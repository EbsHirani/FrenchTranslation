{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Oui.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYqE9QPBA_CQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir(\"drive/My Drive/oui\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tqqv4OfmFvAg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xK4z7qOfJOf9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "fre_input = fre.apply(lambda x: \"<BOS> \" + x.lower()+ \" <EOS>\")\n",
        "fre = fre.apply(lambda x: x.lower() + \" <EOS>\")\n",
        "eng = eng.apply(lambda x: x.lower())\n",
        "\n",
        "def tokenize(data):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(data)\n",
        "  return tokenizer.texts_to_sequences(data), tokenizer\n",
        "def pad_seq(sequences):\n",
        "  padded = pad_sequences(sequences, padding = \"post\")\n",
        "  return padded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLBGsjyOO51a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eng_seq, eng_tok = tokenize(eng)\n",
        "fre_input_seq, fre_tok = tokenize(fre_input)\n",
        "fre_output_seq = fre_tok(fre)\n",
        "eng_seq = pad_seq(eng_seq)\n",
        "fre_output_seq = pad_seq(fre_output_seq)\n",
        "fre_input_seq = pad_seq(fre_input_seq)\n",
        "fre_output_seq = fre_output_seq.reshape(fre_output_seq.shape[0], fre_output_seq.shape[1], 1) #for sparse categorical crsooentropy\n",
        "eng_max = eng_seq.shape[1]\n",
        "fre_max = fre_input_seq.shape[1]\n",
        "eng_vocab_len = len(eng_tok.word_indices)\n",
        "fre_vocab_len = len(fre_tok.word_indices)\n",
        "fre_idx = {id: word for word, id in fre_tok.word_index.items()}\n",
        "index_to_words[0] = '<PAD>' \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v67Of97mZTjC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "LR = 1e-3\n",
        "enc_input = Input(shape = (None,))\n",
        "enc_emb = Embedding(eng_vocab_len, 64)(enc_input)\n",
        "enc_lstm =  CuDNNLSTM(50, return_state = True)\n",
        "_ , state_h, state_c = enc_lstm(emb)\n",
        "enc_states = [state_h, state_c]\n",
        "\n",
        "dec_inputs = Input(shape=(None,))\n",
        "dec_emb = Embedding(fre_vocab_len, 64)(dec_input)\n",
        "dec_lstm = CuSNNLSTM(50, return_sequences = True)\n",
        "dec_out, _ = dec_lstm(dec_emb, initial_state = enc_states)\n",
        "dec_soft = Dense(fre_vocab_len, activation = \"softmax\")\n",
        "dec_pred = dec_soft(dec_out)\n",
        "\n",
        "model = Model([enc_input, dec_input], dec_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oqxu6ZcftCWX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer = Adam(LR), loss = sparse_categorical_crossentropy, metrics = [\"accuracy\"])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXgDUFn3uuMb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit([eng_seq, fre_input_seq], fre_output, batch_size = 128, epochs = 100, callbacks =[EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50),mc = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True)])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}